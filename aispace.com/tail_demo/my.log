root@ip-10-81-13-154:/opt/viv/type-server/logs# tail -1000f webservice-current.log 
    SIXFIVE_CLIENT_TIMEOUT_CLEARS_CONTEXT_ON_INPUT_PROMPT_NON_HEF=true
    SIXFIVE_WORKSPACE_TASK_POOL_BUFFER_SIZE=8
    SIXFIVE_FUNCTION_POOL_MAX_SIZE=2
    SIXFIVE_IDR_ENDPOINT=http\://idr.int/
    SIXFIVE_DEFERRED_LAYOUT_POOL_IDLE_TIMEOUT_MS=60000
    SIXFIVE_KAFKA_ENABLED=true
    SIXFIVE_TREAT_MARKETPLACE_CAPSULES_AS_USER_ENABLED=true
    SIXFIVE_ENFORCE_FEATURE_GATING=false
    SIXFIVE_WORKSPACE_TASK_POOL_CORE_SIZE=2
    SIXFIVE_PDSS_MDE_IOT_USER_VOCAB_ENABLED=false
    SIXFIVE_MS_PREFERENCES_DB_POOL_SIZE=4
    SIXFIVE_EXECUTION_PRUNE_REDUNDANT_PREDECESSOR_ACTIONS=true
    SIXFIVE_DYNAMIC_CAPSULE_CONFIGURATION_EXPIRATION_SECONDS=600
    SIXFIVE_SENSITIVE_CONTENT_EVENTS_FOR_PERMISSIONS=false
    SIXFIVE_CLIENT_RE_PROMPT_TIMEOUT_NON_HEF_IN_SEC=10
    SIXFIVE_CLIENT_REPROMPT_LIMIT_NON_HEF=0
    SIXFIVE_REMOTE_JS_THREAD_COUNT=16
    SIXFIVE_MS_TRANSACTIONS_DB_DATABASE=transactions
    SIXFIVE_MS_TRANSACTIONS_DB_QUERY_TIMEOUT_SECONDS=30
    SIXFIVE_REPOS=/opt/viv/repos
    SIXFIVE_MS_ACCESS_KEYS_DB_PASSWORD=*******
    SIXFIVE_JS_CHECK_INSTRUCTION_COUNT=200000
    SIXFIVE_ACCEPT_LEGACY_DEBUGGING=true
    SIXFIVE_ASSETS_PREFIX_LOCAL=dummy
    SIXFIVE_PG_EXECUTION_LEARNING_DB_QUERY_TIMEOUT_SECONDS=30
    SIXFIVE_DYNAMIC_CAPSULE_CONFIGURATION_ENABLED=false
    SIXFIVE_KAFKA_CONSUMER_STREAMS_ENABLED=true
    SIXFIVE_REVISION_OVERRIDE_ALLOWED=true
    SIXFIVE_PG_CAPSULE_CONFIGURATION_DB_DATABASE=capsuleconfig
    SIXFIVE_MS_ACCESS_KEYS_DB_PASSWORD_KEY=*******
    SIXFIVE_CLIENT_RE_PROMPT_TIMEOUT_HEF_IN_SEC=5
    SIXFIVE_PG_CAN_SUBMISSIONS_SECONDARY_DB_QUERY_TIMEOUT_SECONDS=30
    SIXFIVE_VAULT_AWS_REGION=cn-north-1
    SIXFIVE_ARTIFACT_DOWNLOAD_CONCURRENCY=20
    SIXFIVE_PG_EXECUTION_LEARNING_DB_PASSWORD=*******
    SIXFIVE_CLIENT_TIMEOUT_CLEARS_CONTEXT_ON_LIST_NAVIGATION_PROMPT_NON_HEF=true
    SIXFIVE_CREDSTASH_REGION=us-west-2
    SIXFIVE_CLIENT_TIMEOUT_CLEARS_CONTEXT_ON_LIST_NAVIGATION_PROMPT_HEF=true
    SIXFIVE_CHECKOUT_RELEASE_IN_CAGED_RHINO=false
    SIXFIVE_MDE_DD_IDE_TEST_MODE_ENABLED=false
    SIXFIVE_DEFAULT_CAN_SPEC=bixby-mobile
    SIXFIVE_MS_ACCESS_KEYS_DB_READ_HOSTPORT=int-user-accesskeys-47d64baff074.cluster-ro-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn\:5432
    SIXFIVE_RELEASE_VALIDATION_MINUTES=1
    SIXFIVE_TRACE_FILTER=REMOVE_UNSUPPORTED_STEPS
    SIXFIVE_PG_CAN_SUBMISSIONS_DB_DATABASE=
    SIXFIVE_PYBOS_PORT=8090
    SIXFIVE_PG_CAPSULE_CONFIGURATION_DB_PASSWORD_KEY=*******
    SIXFIVE_CLIENT_TIMEOUT_CLEARS_CONTEXT_ON_FOLLOWUP_PROMPT_NON_HEF=true
    SIXFIVE_CLIENT_RE_PROMPT_TIMEOUT_HEF_IN_SEC_MAP=*\:5
    SIXFIVE_STORE_COUNTRY_KR=kr
    SIXFIVE_GEO_RESOLVER_SERVICE_URL=http\://prod-default-geo-service-local.partner-int\:8888/geocode
    SIXFIVE_EVENT_WRITER_POOL_CORE_SIZE=1
    SIXFIVE_BUILTIN_VERSION_FALLBACK_DISABLED=false
    SIXFIVE_CAPSULE_TRAINING_LIMIT_OVERRIDES=viv.smartThingsVIHApp\:2400,viv.clockApp\:2400,viv.messageApp/ko-KR\:2400,viv.contactApp\:3000,viv.phoneApp\:3000,viv.genie/ko-KR\:2400,viv.bugs/ko-KR\:2400,bixby.melon/ko-KR\:2400,viv.reminderApp\:2400,samsung.airconControl/bixby-appliance-ko-KR\:2400,viv.dianPing/zh-CN\:2400,samsung.speakerCalendarApp/bixby-speaker-ko-KR\:2400,bixby.launcher\:2400,viv.weChat/bixby-mobile-zh-CN\:2400,samsung.speakerCalendar/bixby-speaker-en-US\:2400,samsung.speakerReminderApp/bixby-speaker-en-US\:2400,viv.galleryApp\:2400,viv.calendarApp\:2400,viv.mangoPlate/ko-KR\:2400,viv.weatherNews/zh-CN\:2400,bixby.settingsApp/zh-CN\:2400,viv.youdao/bixby-mobile-zh-CN\:2400,viv.neteaseCloudMusic/bixby-mobile-zh-CN\:2400,viv.baiduMap/bixby-mobile-zh-CN\:2400,bixby.mediaResolver_KR/ko-KR\:2400,viv.cameraApp/bixby-mobile-ko-KR\:2400,bixby.mapResolver_KR\:2400,viv.samsungInternetApp\:2400,viv.systemApp\:2400,bixby.weatherInfoResolver_CN\:2400,samsung.speakerSystemApp\:2400
    SIXFIVE_ENFORCE_SENSITIVE_CONTENT_PERMISSIONS=false
    SIXFIVE_KAFKA_CONSUMER_CONFIG_PATH=/opt/viv/type-server/current/config/kafka-consumer.properties
    SIXFIVE_FUNCTION_TIMEOUT_MS=30000
    SIXFIVE_PG_CAN_SUBMISSIONS_SECONDARY_DB_PASSWORD_KEY=*******
    SIXFIVE_VIEWPORT_POOL_IDLE_TIMEOUT_MS=60000
    SIXFIVE_PG_CAPSULE_CONFIGURATION_DB_USERNAME=viv
    SIXFIVE_PG_CAN_SUBMISSIONS_SECONDARY_DB_USERNAME=
    SIXFIVE_TRUSTED_SAMSUNG_ACCOUNT_AUTH_SERVER_URL_HOSTS=auth.samsungosp.com,us-auth2.samsungosp.com,eu-auth2.samsungosp.com,cn-auth.samsungosp.com
    SIXFIVE_PG_USER_CONFIGURATION_DB_POOL_SIZE=0
    SIXFIVE_SECRET_CACHE_TIME_MINUTES=5
    SIXFIVE_MS_ACCESS_KEYS_DB_HOSTPORT=int-user-accesskeys-47d64baff074.cluster-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn\:5432
    SIXFIVE_VIEWPORT_TASK_QUEUE_SIZE=8
    SIXFIVE_STRICT_RELEASE_REVISION_CHECK=false
    SIXFIVE_DEVICE_DISPATCH_ENABLED_CAN_TYPES=
    SIXFIVE_CLIENT_PROMPT_TIMEOUT_NON_HEF_IN_SEC_MAP=*\:10
    SIXFIVE_MS_TRANSACTIONS_DB_HOSTPORT=int-user-transactions-0f2f27c1ff6e.cluster-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn\:5432
    SIXFIVE_OAUTH_ACCOUNT_LINKING_BASE_URL=https\://us.account.samsung.com/oauth/disclaimer
    SIXFIVE_EVENT_CHANNEL_ASYNC_SIZE=2
    SIXFIVE_PDSS_GAZETTEERS_ENABLED=false
    SIXFIVE_PG_CAPSULE_CONFIGURATION_DB_QUERY_TIMEOUT_SECONDS=30
    SIXFIVE_PG_CAN_SUBMISSIONS_DB_HOSTPORT=
    SIXFIVE_PG_CAN_SUBMISSIONS_DB_QUERY_TIMEOUT_SECONDS=30
    SIXFIVE_PG_EXECUTION_LEARNING_DB_POOL_SIZE=4
    SIXFIVE_EXECUTION_LEARNING_TYPE=COMMUNITY_PREFERENCE_LEARNING
    SIXFIVE_SMART_THINGS_SAMSUNG_ACCOUNT_SCOPE=iot.client
    SIXFIVE_CONVERSATION_DRIVERS_SUPPRESSION_ENABLED=true
    SIXFIVE_CLIENT_RE_PROMPT_LIMIT_HEF_MAP=*\:1
    SIXFIVE_CREDSTASH_ENABLED=false
    SIXFIVE_PG_CAN_SUBMISSIONS_DB_POOL_SIZE=2
    SIXFIVE_DYNAMIC_CAPSULE_CONFIGURATION_ENCRYPTION_KEY_INDEX=-1
    SIXFIVE_CLIENT_TIMEOUT_CLEARS_CONTEXT_ON_CONFIRMATION_PROMPT_HEF=true
    SIXFIVE_LIBRARY_CAPSULE_LIFECYCLE_CONFIG_TIMEOUT_SECONDS=0
    SIXFIVE_SAMSUNG_ACCOUNT_AUTH_SERVER_URL_HOST=auth.samsungosp.com
    SIXFIVE_HARD_CANCEL_META_COMMAND_ENABLED=true
    SIXFIVE_CONVERSATION_FACTORY_SHUTDOWN_MINUTES=5
    SIXFIVE_MS_PREFERENCES_DB_PASSWORD=*******
    SIXFIVE_USE_CACHED_SECRETS=true
    SIXFIVE_ASSETS_URI=s3\://bixby-developer-assets/can_submissions
    SIXFIVE_WEB_CACHE_ROOT=/opt/viv/type-server/current/bin/../type-sandbox/src/test/data/webcache
    SIXFIVE_PG_USER_CONFIGURATION_DB_PASSWORD_KEY=*******
    SIXFIVE_VAULT_ROLE=cnn1_int_bixby_exl
    SIXFIVE_MS_ACCESS_KEYS_DB_POOL_SIZE=2
    SIXFIVE_PG_CAN_SUBMISSIONS_DB_PASSWORD_KEY=*******
    SIXFIVE_CAN_SUBMISSIONS_POOL_CORE_SIZE=1
    SIXFIVE_CLIENT_PROMPT_TIMEOUT_HEF_IN_SEC_MAP=*\:7
    SIXFIVE_OAUTH_PROXY_URL=https\://{safe_capsule_id}.oauth.aibixby.com
    SIXFIVE_PG_USER_CONFIGURATION_DB_HOSTPORT=int-userconfiguration-710b9075064a.cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn\:5432
    SIXFIVE_USER_CONFIGURATION_CONVERSATIONS_ENABLED=true
    SIXFIVE_DROP_METRIC_EVENTS=true
    SIXFIVE_KAFKA_CONSUMERS_PER_MESSAGE_GROUP=4
    SIXFIVE_EVENT_WRITER_BUFFER_SIZE=64
    SIXFIVE_MS_PREFERENCES_DB_USERNAME=viv
    SIXFIVE_EVENT_WRITER_POOL_MAX_SIZE=2
    # Unset items below (commented so defaults apply)
    #SIXFIVE_INSTANCE_ID=
    #SIXFIVE_ARTIFACT_READ_ONLY_REPO_PATH=
    #SIXFIVE_ARTIFACT_READ_WRITE_REPO_PATH=
    #SIXFIVE_SPS_PARTNER_URL=
    #SIXFIVE_SPS_PARTNER_KEY=
    #SIXFIVE_SPREEDLY_ENVIRONMENT_KEY_CREDSTASH_KEY=
    #SIXFIVE_SPREEDLY_ACCESS_SECRET_CREDSTASH_KEY=
    #SIXFIVE_CAN_SUBMISSIONS_RUN_ONCE_CONFIG=
    #SIXFIVE_CAN_SUBMISSIONS_MARKETPLACE_BUCKET=
    #SIXFIVE_CAN_SUBMISSIONS_ENV=
    #SIXFIVE_RELEASE_CONFIG_EVALUATION_INPUT=
    #SIXFIVE_STORY_RUNNER_VERSION=
    #SIXFIVE_STORY_RUNNER_AUTH_TOKEN=
    #SIXFIVE_CES_SERVER_HOST=
    #SIXFIVE_NODE_EXTRA_CA_CERTS=
    #SIXFIVE_STORY_RUNNER_CAPSULES_PATH=
    #SIXFIVE_STORY_RUNNER_CAN_TYPE=
    #SIXFIVE_PRE_LOAD_RELEASE_CAN_TYPES=
    #SIXFIVE_PRE_LOAD_RELEASE_CAN_TYPES_REGEX=
    #SIXFIVE_CUSTOM_CAPSULE_CLASSIFIER_TRAINER_CONFIG=
    #SIXFIVE_USER_CAPSULE_CLASSIFIER_RETRAINER_CONFIG=
    #SIXFIVE_ENQUEUE_EXECUTION_LEARNING_REQUESTS_CONFIG=
    #SIXFIVE_COUNT_PENDING_EXECUTION_LEARNING_MODELS_CONFIG=
    #SIXFIVE_SUPPORTED_CAN_TYPES=
    #SIXFIVE_SUPPORTED_CAN_TYPES_REGEX=
    #SIXFIVE_DEPLOYMENT_JURISDICTION=
    #SIXFIVE_OAUTH_BLACKLISTED_AUTH_URLS=
    #SIXFIVE_OAUTH_BLACKLIST_EXEMPT_CAPSULES=
    #SIXFIVE_OAUTH_SIGNATURE_SECRET_CREDSTASH_KEY=
    #SIXFIVE_OAUTH_VERIFIER_ENCRYPTION_SECRET=
    #SIXFIVE_DEBUGGING_TOKEN_PUBLIC_KEY_CREDSTASH_KEY=
    #SIXFIVE_SMART_THINGS_SAMSUNG_ACCOUNT_CLIENT_SECRET_CREDSTASH_KEY=
    #SIXFIVE_DYNAMIC_BLOCKLIST=
    #SIXFIVE_DYNAMIC_CAPSULE_CONFIGURATION_ENCRYPTION_KEY_PREFIX=
    #SIXFIVE_CAPSULE_LOCK_IN_TIMEOUT_PAGE_DIALOG_MAP=
    #SIXFIVE_SECURE_BIXBY_USER_ID_SECRET_CREDSTASH_KEY=
    #SIXFIVE_EMERGENCY_GOALS=
    #SIXFIVE_APPCONTEXT_APP_CAPSULE_ID_MAPPING=
    #SIXFIVE_SECOND_PARTY_ORGS=
    #SIXFIVE_SECOND_PARTY_CAPSULES=

2020-05-26 06:52:54,749 MPlat INFO  [main] [] (,,,) c.s.c.l.c.VaultManager - Vault login successful
2020-05-26 06:52:54,751 MPlat INFO  [main] [] (,,,) c.s.c.l.c.VaultManager - Scheduled to run vault token renewal in 82800 seconds
2020-05-26 06:52:54,899 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-accesskeys-47d64baff074.cluster-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],accesskeys,viv,2>
2020-05-26 06:52:54,906 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-accesskeys-47d64baff074.cluster-ro-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],accesskeys,viv,2>
2020-05-26 06:52:54,920 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-transactions-0f2f27c1ff6e.cluster-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],transactions,viv,2>
2020-05-26 06:52:54,922 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-transactions-0f2f27c1ff6e.cluster-ro-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],transactions,viv,2>
2020-05-26 06:52:54,931 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-preferences-b829c3d92834.cluster-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],preferences,viv,4>
2020-05-26 06:52:54,934 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-user-preferences-b829c3d92834.cluster-ro-cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],preferences,viv,4>
2020-05-26 06:52:54,941 MPlat INFO  [main] [] (,,,) c.s.c.c.d.SQLConnectionProvider - Initialized SQLConnectionProvider <[int-userconfiguration-710b9075064a.cmga49hjxyra.rds.cn-north-1.amazonaws.com.cn:5432],userconfiguration,viv,0>
2020-05-26 06:52:55,012 M     INFO  [main] [] (,,,) i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
        bearer.auth.token = [hidden]
        schema.registry.url = [http://10.81.12.217:8081, http://10.81.16.206:8081, http://10.81.13.41:8081, http://10.81.15.87:8081, http://10.81.16.42:8081, http://10.81.13.199:8081]
        basic.auth.user.info = [hidden]
        auto.register.schemas = true
        max.schemas.per.subject = 1000
        basic.auth.credentials.source = URL
        schema.registry.basic.auth.user.info = [hidden]
        bearer.auth.credentials.source = STATIC_TOKEN
        value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
        key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-05-26 06:52:55,024 M     INFO  [main] [] (,,,) i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
        bearer.auth.token = [hidden]
        schema.registry.url = [http://10.81.12.217:8081, http://10.81.16.206:8081, http://10.81.13.41:8081, http://10.81.15.87:8081, http://10.81.16.42:8081, http://10.81.13.199:8081]
        basic.auth.user.info = [hidden]
        auto.register.schemas = true
        max.schemas.per.subject = 1000
        basic.auth.credentials.source = URL
        schema.registry.basic.auth.user.info = [hidden]
        bearer.auth.credentials.source = STATIC_TOKEN
        specific.avro.reader = false
        value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
        key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-05-26 06:52:55,024 M     INFO  [main] [] (,,,) i.c.k.s.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
        bearer.auth.token = [hidden]
        schema.registry.url = [http://10.81.12.217:8081, http://10.81.16.206:8081, http://10.81.13.41:8081, http://10.81.15.87:8081, http://10.81.16.42:8081, http://10.81.13.199:8081]
        basic.auth.user.info = [hidden]
        auto.register.schemas = true
        max.schemas.per.subject = 1000
        basic.auth.credentials.source = URL
        schema.registry.basic.auth.user.info = [hidden]
        bearer.auth.credentials.source = STATIC_TOKEN
        value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
        key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-05-26 06:52:55,024 M     INFO  [main] [] (,,,) i.c.k.s.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
        bearer.auth.token = [hidden]
        schema.registry.url = [http://10.81.12.217:8081, http://10.81.16.206:8081, http://10.81.13.41:8081, http://10.81.15.87:8081, http://10.81.16.42:8081, http://10.81.13.199:8081]
        basic.auth.user.info = [hidden]
        auto.register.schemas = true
        max.schemas.per.subject = 1000
        basic.auth.credentials.source = URL
        schema.registry.basic.auth.user.info = [hidden]
        bearer.auth.credentials.source = STATIC_TOKEN
        specific.avro.reader = false
        value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
        key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2020-05-26 06:52:55,061 M     INFO  [main] [] (,,,) o.a.k.s.StreamsConfig - StreamsConfig values: 
        application.id = viv.CommunityPreferenceLearning
        application.server = 
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        buffered.records.per.partition = 1000
        cache.max.bytes.buffering = 10485760
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154
        commit.interval.ms = 100
        connections.max.idle.ms = 540000
        default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
        default.key.serde = class io.confluent.kafka.streams.serdes.avro.GenericAvroSerde
        default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
        default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
        default.value.serde = class io.confluent.kafka.streams.serdes.avro.GenericAvroSerde
        max.task.idle.ms = 0
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        num.standby.replicas = 0
        num.stream.threads = 4
        partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
        poll.ms = 100
        processing.guarantee = exactly_once
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        replication.factor = 1
        request.timeout.ms = 60000
        retries = 0
        retry.backoff.ms = 100
        rocksdb.config.setter = null
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        state.cleanup.delay.ms = 600000
        state.dir = /tmp/kafka-streams
        topology.optimization = none
        upgrade.from = null
        windowstore.changelog.additional.retention.ms = 86400000

2020-05-26 06:52:55,128 M     INFO  [main] [] (,,,) o.a.k.c.a.AdminClientConfig - AdminClientConfig values: 
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-admin
        connections.max.idle.ms = 300000
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retries = 5
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS

2020-05-26 06:52:55,215 M     WARN  [main] [] (,,,) o.a.k.c.a.AdminClientConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,217 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,217 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,218 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975216
2020-05-26 06:52:55,220 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-1] Creating restore consumer client
2020-05-26 06:52:55,229 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = none
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-1-restore-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,332 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,332 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,332 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,332 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975332
2020-05-26 06:52:55,364 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-1] Creating consumer client
2020-05-26 06:52:55,367 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-1-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = viv.CommunityPreferenceLearning
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,438 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,439 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-05-26 06:52:55,439 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retries' was supplied but isn't a known config.
2020-05-26 06:52:55,439 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,439 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,439 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975439
2020-05-26 06:52:55,443 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-2] Creating restore consumer client
2020-05-26 06:52:55,444 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = none
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-2-restore-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,473 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,473 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,473 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,473 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975473
2020-05-26 06:52:55,481 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-2] Creating consumer client
2020-05-26 06:52:55,482 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-2-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = viv.CommunityPreferenceLearning
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,531 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,531 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-05-26 06:52:55,531 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retries' was supplied but isn't a known config.
2020-05-26 06:52:55,532 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,532 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,532 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975531
2020-05-26 06:52:55,533 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-3] Creating restore consumer client
2020-05-26 06:52:55,534 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = none
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-3-restore-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,571 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,571 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,571 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,571 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975571
2020-05-26 06:52:55,579 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-3] Creating consumer client
2020-05-26 06:52:55,580 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-3-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = viv.CommunityPreferenceLearning
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,630 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,633 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-05-26 06:52:55,634 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retries' was supplied but isn't a known config.
2020-05-26 06:52:55,634 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,635 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,635 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975634
2020-05-26 06:52:55,638 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-4] Creating restore consumer client
2020-05-26 06:52:55,639 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = none
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-4-restore-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = null
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,656 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,656 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,657 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,657 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975656
2020-05-26 06:52:55,664 M     INFO  [main] [] (,,,) o.a.k.s.p.i.StreamThread - stream-thread [viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-4] Creating consumer client
2020-05-26 06:52:55,666 M     INFO  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - ConsumerConfig values: 
        allow.auto.create.topics = true
        auto.commit.interval.ms = 5000
        auto.offset.reset = earliest
        bootstrap.servers = [10.81.12.217:9092, 10.81.16.206:9092, 10.81.13.41:9092, 10.81.15.87:9092, 10.81.16.42:9092, 10.81.13.199:9092]
        check.crcs = true
        client.dns.lookup = default
        client.id = viv.CommunityPreferenceLearning-1-ip-10-81-13-154-StreamThread-4-consumer
        client.rack = 
        connections.max.idle.ms = 540000
        default.api.timeout.ms = 60000
        enable.auto.commit = false
        exclude.internal.topics = true
        fetch.max.bytes = 52428800
        fetch.max.wait.ms = 500
        fetch.min.bytes = 1
        group.id = viv.CommunityPreferenceLearning
        group.instance.id = null
        heartbeat.interval.ms = 30000
        interceptor.classes = []
        internal.leave.group.on.close = false
        isolation.level = read_committed
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        max.partition.fetch.bytes = 1048576
        max.poll.interval.ms = 14400000
        max.poll.records = 1
        metadata.max.age.ms = 300000
        metric.reporters = [com.sixfive.can.messages.metrics.KafkaDropwizardReporter]
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
        receive.buffer.bytes = 65536
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 60000
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.mechanism = GSSAPI
        security.protocol = PLAINTEXT
        send.buffer.bytes = 131072
        session.timeout.ms = 120000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.endpoint.identification.algorithm = https
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLS
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2020-05-26 06:52:55,687 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'schema.registry.url' was supplied but isn't a known config.
2020-05-26 06:52:55,688 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retry.backoff.ms' was supplied but isn't a known config.
2020-05-26 06:52:55,688 M     WARN  [main] [] (,,,) o.a.k.c.c.ConsumerConfig - The configuration 'admin.retries' was supplied but isn't a known config.
2020-05-26 06:52:55,689 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka version: 5.3.0-ccs
2020-05-26 06:52:55,689 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka commitId: 29b8bc3c33c75c2a
2020-05-26 06:52:55,690 M     INFO  [main] [] (,,,) o.a.k.c.u.AppInfoParser - Kafka startTimeMs: 1590475975689
2020-05-26 06:52:55,694 MPlat INFO  [main] [] (,,,) c.s.u.d.RuntimeEnvironment - Deployment: mode=Production, cores=2, memory=3569352704, buildId=Six5-20200514T232843Z-r20f-1794-f7b9744, distHome=/opt/viv/type-server/dir/20200522T041019Z, localHostname=ip-10-81-13-154, tempDir=/opt/viv/type-server/dir/20200522T041019Z/tmp
2020-05-26 06:52:55,695 MPlat INFO  [main] [] (,,,) c.s.u.d.RuntimeEnvironment - Environment: cwd=/opt/viv/type-server/dir/20200522T041019Z, platform=Linux, jvm=11 (OpenJDK 64-Bit Server VM, 11.0.4+11-LTS), java-opts=[-XX:+PrintCommandLineFlags, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=/opt/viv, -XX:+ExitOnOutOfMemoryError, -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005, -Dcom.sun.management.jmxremote.port=5010, -Dcom.sun.management.jmxremote.rmi.port=5010, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Djava.rmi.server.hostname=127.0.0.1, -Dfile.encoding=UTF-8, -Xmx3403m, -Xms2634m]
2020-05-26 06:52:55,699 M     INFO  [main] [] (,,,) i.d.server.ServerFactory - Starting ExecutionLearningService
2020-05-26 06:52:55,817 M     INFO  [main] [] (,,,) o.e.j.s.SetUIDListener - Opened application@ffc9278{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
2020-05-26 06:52:55,818 M     INFO  [main] [] (,,,) o.e.j.s.SetUIDListener - Opened admin@1d9643e6{HTTP/1.1,[http/1.1]}{0.0.0.0:8081}
2020-05-26 06:52:55,818 M     DEBUG [main] [] (,,,) i.d.l.s.LifecycleEnvironment - managed objects = [io.dropwizard.lifecycle.ExecutorServiceManager@40df1311(TimeBoundHealthCheck-pool-%d), io.dropwizard.metrics.ScheduledReporterManager@37248b2c, com.sixfive.can.lifecycle.credentials.CachingSecretAccessor@f002a22, com.sixfive.can.discovery.ReleaseManager@6bf597a9, com.sixfive.can.messages.publish.MessagePublishingManager@538a465d, com.sixfive.can.messages.subscribe.MessageQueueSubscriptionManager@4064daeb]
2020-05-26 06:52:55,822 M     INFO  [main] [] (,,,) o.e.jetty.server.Server - jetty-9.4.18.v20190429; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.4+11-LTS
2020-05-26 06:53:02,834 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.rideShareResolver_KR/0.3.0/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.rideShareResolver_KR/0.3.0/capsule.tgz
2020-05-26 06:53:03,494 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.neshelperFR/0.3.0/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.neshelperFR/0.3.0/capsule.tgz
2020-05-26 06:53:03,535 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/samsung.bixbyChatTV_A/1.0.7/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/samsung.bixbyChatTV_A/1.0.7/capsule.tgz
2020-05-26 06:53:03,553 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/samsung.bixbyChatTV_C/1.0.7/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/samsung.bixbyChatTV_C/1.0.7/capsule.tgz
2020-05-26 06:53:03,569 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/samsung.bixbyChatTV_B/1.0.7/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/samsung.bixbyChatTV_B/1.0.7/capsule.tgz
2020-05-26 06:53:03,613 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.netflixGB/1.1.9/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.netflixGB/1.1.9/capsule.tgz
2020-05-26 06:53:03,631 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/viv.mediaResolverFR/0.2.4/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/viv.mediaResolverFR/0.2.4/capsule.tgz
2020-05-26 06:53:03,657 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.restaurantResolver_KR/0.1.6/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.restaurantResolver_KR/0.1.6/capsule.tgz
2020-05-26 06:53:03,705 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.netflixDE/0.0.8/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.netflixDE/0.0.8/capsule.tgz
2020-05-26 06:53:03,731 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/viv.emailResolverFR/0.1.2/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/viv.emailResolverFR/0.1.2/capsule.tgz
2020-05-26 06:53:03,765 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.flightResolver_KR/0.1.5/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.flightResolver_KR/0.1.5/capsule.tgz
2020-05-26 06:53:03,781 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.lojaSamsung/0.1.11/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.lojaSamsung/0.1.11/capsule.tgz
2020-05-26 06:53:03,795 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.covidinfoBR/0.2.8/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.covidinfoBR/0.2.8/capsule.tgz
2020-05-26 06:53:03,821 MPlat DEBUG [artifact-downloader-0] [] (,,,) com.sixfive.util.aws.S3 - Getting file: s3://bixby-submissions/prd/live/capsules/master/bixby.liivon/0.2.4/capsule.tgz -> ../../../repos/cache/capsule/compressedSource/master/bixby.liivon/0.2.4/capsule.tgz
2020-05-26 06:53:03,844 MPlat INFO  [main] [] (,,,) c.s.c.s.a.ArtifactDownloader - Downloaded 1067 artifacts (capsule source) in 1 secs
2020-05-26 06:53:03,852 M     ERROR [main] [] (,,,) i.d.cli.ServerCommand - Unable to start server, shutting down
java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.sixfive.can.submissions.artifacts.ArtifactException: Failed to download CAPSULE_SOURCE from 's3://bixby-submissions/prd/live/capsules/master/bixby.rideShareResolver_KR/0.3.0/capsule.tgz': The specified key does not exist.
        at com.google.common.base.Throwables.propagate(Throwables.java:241)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloader.download(ArtifactDownloader.java:157)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloader.downloadCapsuleSource(ArtifactDownloader.java:69)
        at com.sixfive.can.discovery.Release.checkout(Release.java:331)
        at com.sixfive.can.discovery.ReleaseManager.buildRelease(ReleaseManager.java:234)
        at com.sixfive.can.discovery.ReleaseManager.checkoutDefaultRelease(ReleaseManager.java:192)
        at com.sixfive.can.discovery.ReleaseManager.start(ReleaseManager.java:138)
        at io.dropwizard.lifecycle.JettyManaged.doStart(JettyManaged.java:27)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:167)
        at org.eclipse.jetty.server.Server.start(Server.java:418)
        at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:119)
        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:113)
        at org.eclipse.jetty.server.Server.doStart(Server.java:382)
        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)
        at io.dropwizard.cli.ServerCommand.run(ServerCommand.java:53)
        at io.dropwizard.cli.EnvironmentCommand.run(EnvironmentCommand.java:44)
        at io.dropwizard.cli.ConfiguredCommand.run(ConfiguredCommand.java:87)
        at io.dropwizard.cli.Cli.run(Cli.java:78)
        at io.dropwizard.Application.run(Application.java:93)
        at com.sixfive.can.user.training.ExecutionLearningService.main(ExecutionLearningService.java:19)
Caused by: java.util.concurrent.ExecutionException: com.sixfive.can.submissions.artifacts.ArtifactException: Failed to download CAPSULE_SOURCE from 's3://bixby-submissions/prd/live/capsules/master/bixby.rideShareResolver_KR/0.3.0/capsule.tgz': The specified key does not exist.
        at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)
        at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloader.download(ArtifactDownloader.java:152)
        ... 19 common frames omitted
Caused by: com.sixfive.can.submissions.artifacts.ArtifactException: Failed to download CAPSULE_SOURCE from 's3://bixby-submissions/prd/live/capsules/master/bixby.rideShareResolver_KR/0.3.0/capsule.tgz': The specified key does not exist.
        at com.sixfive.can.submissions.artifacts.ArtifactDownloadSynchronizer.lambda$download$0(ArtifactDownloadSynchronizer.java:81)
        at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloadSynchronizer.download(ArtifactDownloadSynchronizer.java:40)
        at com.sixfive.can.submissions.artifacts.Artifact.download(Artifact.java:148)
        at com.sixfive.can.submissions.service.CapsuleSourceDownloader.downloadCapsule(CapsuleSourceDownloader.java:46)
        at com.sixfive.can.submissions.service.CapsuleSourceDownloader.downloadCapsule(CapsuleSourceDownloader.java:32)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloader.lambda$downloadCapsuleSource$0(ArtifactDownloader.java:58)
        at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1736)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 9A4C35AD6CFDF0BA; S3 Extended Request ID: 8HmUTlXPtMH3aVuhr5egnnEQFdQ6HT+o5ntjlLHTNXjAxqu+487FrTm7CSJWhug+PcLdGFmsTEY=)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)
        at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)
        at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)
        at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1467)
        at com.amazonaws.services.s3.AmazonS3Client$3.getS3ObjectStream(AmazonS3Client.java:1558)
        at com.amazonaws.services.s3.internal.ServiceUtils.retryableDownloadS3ObjectToFile(ServiceUtils.java:398)
        at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1554)
        at com.sixfive.util.aws.S3.getFile(S3.java:127)
        at com.sixfive.can.submissions.store.S3CanSubmissionDataStore.downloadArtifact(S3CanSubmissionDataStore.java:41)
        at com.sixfive.can.submissions.artifacts.ArtifactDownloadSynchronizer.lambda$download$0(ArtifactDownloadSynchronizer.java:50)
        ... 10 common frames omitted